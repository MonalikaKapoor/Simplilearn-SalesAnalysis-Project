{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1724df",
   "metadata": {},
   "source": [
    "Sales Analysis - Full Python Script\n",
    "Author: Monalika Kapoor\n",
    "Purpose: \n",
    " - Performs data wrangling (missing values, parsing dates)\n",
    " - Normalizes numeric columns (Min-Max)\n",
    " - Performs descriptive stats for Sales and Unit\n",
    " - Aggregates by State and Group and by time (daily/weekly/monthly/quarterly)\n",
    " - Produces and saves visualizations and CSV outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebb205c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# IMPORT LIBRARIES\n",
    "# ----------------------------------------------\n",
    "# Library\tPurpose\n",
    "# pandas\tLoad & manipulate CSV data\n",
    "# numpy\tNumerical computation\n",
    "# Pathlib\tClean file path handling\n",
    "# MinMaxScaler\tNormalization of numeric columns\n",
    "# matplotlib/seaborn\tData visualization & charts\n",
    "# DateFormatter\tFormat time-based axis in plots\n",
    "# warnings\tSuppress irrelevant warnings\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# File paths\n",
    "from pathlib import Path\n",
    "\n",
    "# Normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "# Ignore warnings for cleaner outputs\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bc09a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully!\n",
      "Shape: (7560, 6)\n",
      "Columns: ['Date', 'Time', 'State', 'Group', 'Unit', 'Sales']\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# LOAD DATA\n",
    "# ----------------------------------------------   \n",
    "DATA_PATH = Path(\"AusApparalSales4thQrt2020.csv\")# <-- using your uploaded CSV\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "OUTPUT_DIR = Path(\"sales_analysis_output\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"Data Loaded Successfully!\")\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7d497d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected Columns:\n",
      "Date: Date\n",
      "Sales: Sales\n",
      "Units: Unit\n",
      "State: State\n",
      "Group: Group\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# DETECT COLUMNS AUTOMATICALLY\n",
    "# ----------------------------------------------\n",
    "def detect_columns(df):\n",
    "    cols = df.columns\n",
    "    date_col = next((c for c in cols if \"date\" in c.lower()), None)\n",
    "    sales_col = next((c for c in cols if c.lower() == \"sales\"), None)\n",
    "    unit_col = next((c for c in cols if \"unit\" in c.lower()), None)\n",
    "    state_col = next((c for c in cols if \"state\" in c.lower()), None)\n",
    "    group_col = next((c for c in cols if \"group\" in c.lower()), None)\n",
    "    return date_col, sales_col, unit_col, state_col, group_col\n",
    "\n",
    "DATE_COL, SALES_COL, UNIT_COL, STATE_COL, GROUP_COL = detect_columns(df)\n",
    "print(\"\\nDetected Columns:\")\n",
    "print(\"Date:\", DATE_COL)\n",
    "print(\"Sales:\", SALES_COL)\n",
    "print(\"Units:\", UNIT_COL)\n",
    "print(\"State:\", STATE_COL)\n",
    "print(\"Group:\", GROUP_COL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ffbe9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values Summary:\n",
      "Date          0\n",
      "Time          0\n",
      "State         0\n",
      "Group         0\n",
      "Unit          0\n",
      "Sales         0\n",
      "ParsedDate    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------------------------\n",
    "# DATA WRANGLING\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Parse dates\n",
    "df[\"ParsedDate\"] = pd.to_datetime(df[DATE_COL], errors=\"coerce\")\n",
    "\n",
    "# Missing value check\n",
    "print(\"\\nMissing Values Summary:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Fill numeric missing with median\n",
    "for col in df.select_dtypes(include=[np.number]).columns:\n",
    "    df[col].fillna(df[col].median(), inplace=True)\n",
    "\n",
    "# Fill categorical missing with 'Unknown'\n",
    "for col in df.select_dtypes(include=[\"object\"]).columns:\n",
    "    df[col].fillna(\"Unknown\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02de99fb",
   "metadata": {},
   "source": [
    "## Data Wrangling Summary\n",
    "\n",
    "The dataset required several preprocessing steps to ensure accuracy and consistency:\n",
    "\n",
    "### 1. Handling Missing Values\n",
    "- **Numeric columns** were filled using the **median** because the median is robust against extreme values and prevents distortion of the data distribution.\n",
    "- **Categorical columns** were filled with `\"Unknown\"` to avoid dropping rows and to preserve category-level analysis.\n",
    "\n",
    "### 2. Date Parsing\n",
    "The date column was converted into a proper `datetime` format using `pd.to_datetime()`. This enables reliable:\n",
    "- sorting\n",
    "- time-based grouping (daily, weekly, monthly)\n",
    "- trend analysis\n",
    "\n",
    "### 3. Normalization (Min–Max Scaling)\n",
    "MinMaxScaler was used to transform all numeric fields between **0 and 1**:\n",
    "\\[\n",
    "X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "\\]\n",
    "This is ideal for:\n",
    "- visual comparison\n",
    "- distance-based models\n",
    "- scale-sensitive analysis\n",
    "\n",
    "### 4. Why GroupBy is Useful\n",
    "The `groupby()` function is essential for performing:\n",
    "- state-level aggregation  \n",
    "- customer-group aggregation  \n",
    "- time-level rollups  \n",
    "It simplifies complex relational summaries into clean, readable outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7377e00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# NORMALIZATION (Min–Max Scaling)\n",
    "# ----------------------------------------------\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Reset index to ensure unique values before concat\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Automatically select numeric columns, ensuring uniqueness\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove already normalized columns if they exist\n",
    "numeric_cols = [c for c in numeric_cols if not c.endswith('_norm')]\n",
    "\n",
    "if len(numeric_cols) > 0:\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized = scaler.fit_transform(df[numeric_cols])\n",
    "    \n",
    "    # Create DataFrame for normalized columns with proper alignment\n",
    "    normalized_df = pd.DataFrame(normalized, \n",
    "                                 columns=[f\"{c}_norm\" for c in numeric_cols],\n",
    "                                 index=df.index)  # aligns with original df\n",
    "    \n",
    "    # Concatenate normalized columns to the original DataFrame\n",
    "    df = pd.concat([df, normalized_df], axis=1)\n",
    "else:\n",
    "    print(\"No numeric columns found for normalization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9322e3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Descriptive Statistics – Sales:\n",
      "count      7560.000000\n",
      "mean      45013.558201\n",
      "std       32253.506944\n",
      "min        5000.000000\n",
      "25%       20000.000000\n",
      "50%       35000.000000\n",
      "75%       65000.000000\n",
      "max      162500.000000\n",
      "Name: Sales, dtype: float64\n",
      "\n",
      "Descriptive Statistics – Units:\n",
      "count    7560.000000\n",
      "mean       18.005423\n",
      "std        12.901403\n",
      "min         2.000000\n",
      "25%         8.000000\n",
      "50%        14.000000\n",
      "75%        26.000000\n",
      "max        65.000000\n",
      "Name: Unit, dtype: float64\n",
      "\n",
      "Top States by Sales:\n",
      " State\n",
      "VIC    105565000\n",
      "NSW     74970000\n",
      "SA      58857500\n",
      "QLD     33417500\n",
      "TAS     22760000\n",
      "Name: Sales, dtype: int64\n",
      "\n",
      "Bottom States by Sales:\n",
      " State\n",
      "SA     58857500\n",
      "QLD    33417500\n",
      "TAS    22760000\n",
      "NT     22580000\n",
      "WA     22152500\n",
      "Name: Sales, dtype: int64\n",
      "\n",
      "Top Groups by Sales:\n",
      " Group\n",
      "Men        85750000\n",
      "Women      85442500\n",
      "Kids       85072500\n",
      "Seniors    84037500\n",
      "Name: Sales, dtype: int64\n",
      "\n",
      "Bottom Groups by Sales:\n",
      " Group\n",
      "Men        85750000\n",
      "Women      85442500\n",
      "Kids       85072500\n",
      "Seniors    84037500\n",
      "Name: Sales, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# DESCRIPTIVE STATISTICS\n",
    "# ----------------------------------------------\n",
    "print(\"\\nDescriptive Statistics – Sales:\")\n",
    "print(df[SALES_COL].describe())\n",
    "\n",
    "print(\"\\nDescriptive Statistics – Units:\")\n",
    "print(df[UNIT_COL].describe())\n",
    "\n",
    "# ----------------------------------------------\n",
    "# HIGHEST & LOWEST SALES (STATE + GROUP)\n",
    "# ----------------------------------------------\n",
    "state_sales = df.groupby(STATE_COL)[SALES_COL].sum().sort_values(ascending=False)\n",
    "group_sales = df.groupby(GROUP_COL)[SALES_COL].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop States by Sales:\\n\", state_sales.head())\n",
    "print(\"\\nBottom States by Sales:\\n\", state_sales.tail())\n",
    "print(\"\\nTop Groups by Sales:\\n\", group_sales.head())\n",
    "print(\"\\nBottom Groups by Sales:\\n\", group_sales.tail())\n",
    "\n",
    "state_sales.to_csv(OUTPUT_DIR/\"state_total_sales.csv\")\n",
    "group_sales.to_csv(OUTPUT_DIR/\"group_total_sales.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "628c1ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time-Based Reports Generated!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# TIME-BASED REPORTING\n",
    "# ----------------------------------------------\n",
    "df = df.set_index(\"ParsedDate\")\n",
    "\n",
    "daily = df[SALES_COL].resample(\"D\").sum()\n",
    "weekly = df[SALES_COL].resample(\"W\").sum()\n",
    "monthly = df[SALES_COL].resample(\"M\").sum()\n",
    "quarterly = df[SALES_COL].resample(\"Q\").sum()\n",
    "\n",
    "daily.to_csv(OUTPUT_DIR/\"daily_sales.csv\")\n",
    "weekly.to_csv(OUTPUT_DIR/\"weekly_sales.csv\")\n",
    "monthly.to_csv(OUTPUT_DIR/\"monthly_sales.csv\")\n",
    "quarterly.to_csv(OUTPUT_DIR/\"quarterly_sales.csv\")\n",
    "\n",
    "print(\"\\nTime-Based Reports Generated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cb6c859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hourly sales analysis completed!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# TIME OF DAY ANALYSIS (Peak & Off-Peak Hours)\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Extract hour from original date column\n",
    "df[\"Hour\"] = df.index.hour\n",
    "\n",
    "hourly_sales = df.groupby(\"Hour\")[SALES_COL].sum()\n",
    "\n",
    "# Save results\n",
    "hourly_sales.to_csv(OUTPUT_DIR/\"hourly_sales.csv\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,5))\n",
    "hourly_sales.plot(kind=\"line\", marker=\"o\")\n",
    "plt.title(\"Hourly Sales Trend\")\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Total Sales\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR/\"hourly_sales_trend.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nHourly sales analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78305491",
   "metadata": {},
   "source": [
    "## Time-of-Day Analysis\n",
    "\n",
    "The hour component was extracted from the timestamp to understand purchase patterns throughout the day.\n",
    "\n",
    "### Key Insights:\n",
    "- **Peak Hours** → periods of highest sales activity.\n",
    "- **Off-Peak Hours** → periods of low activity, useful for staffing and marketing adjustments.\n",
    "\n",
    "This analysis helps identify when customers are most active, allowing businesses to optimize:\n",
    "- store staffing\n",
    "- marketing notifications\n",
    "- discount timings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b67f779c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekly / Monthly / Quarterly plots saved!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# WEEKLY, MONTHLY, QUARTERLY VISUALIZATIONS\n",
    "# ----------------------------------------------\n",
    "\n",
    "# Weekly Sales Trend\n",
    "plt.figure(figsize=(10,5))\n",
    "weekly.plot()\n",
    "plt.title(\"Weekly Sales Trend\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR/\"weekly_sales_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# Monthly Sales Trend\n",
    "plt.figure(figsize=(10,5))\n",
    "monthly.plot()\n",
    "plt.title(\"Monthly Sales Trend\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR/\"monthly_sales_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# Quarterly Sales Trend\n",
    "plt.figure(figsize=(10,5))\n",
    "quarterly.plot()\n",
    "plt.title(\"Quarterly Sales Trend\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR/\"quarterly_sales_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Weekly / Monthly / Quarterly plots saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0680eb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Visualizations Saved!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# VISUALIZATION\n",
    "# ----------------------------------------------\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# 1) State-wise sales by group\n",
    "pivot_table = df.pivot_table(values=SALES_COL, index=STATE_COL, columns=GROUP_COL, aggfunc=\"sum\", fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "pivot_table.plot(kind=\"bar\")\n",
    "plt.title(\"State-wise Sales by Group\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR/\"state_group_sales.png\")\n",
    "plt.close()\n",
    "\n",
    "# 2) Total sales by group\n",
    "plt.figure(figsize=(8,5))\n",
    "group_sales.plot(kind=\"bar\")\n",
    "plt.title(\"Total Sales by Group\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR/\"group_sales.png\")\n",
    "plt.close()\n",
    "\n",
    "# 3) Daily Sales Trend\n",
    "plt.figure(figsize=(10,5))\n",
    "daily.plot()\n",
    "plt.title(\"Daily Sales Trend\")\n",
    "plt.ylabel(\"Sales\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR/\"daily_sales_plot.png\")\n",
    "plt.close()\n",
    "\n",
    "# 4) Boxplot (Sales & Units)\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(data=df[[SALES_COL, UNIT_COL]])\n",
    "plt.title(\"Boxplot - Sales & Units\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR/\"boxplot_sales_units.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nVisualizations Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8917d08",
   "metadata": {},
   "source": [
    "## Boxplot Interpretation\n",
    "\n",
    "The boxplot compares the distribution of **Sales** and **Units Sold**.\n",
    "\n",
    "- The **box** represents the interquartile range (middle 50% of data).\n",
    "- The **line** inside the box shows the median value.\n",
    "- **Whiskers** represent variability outside the upper and lower quartiles.\n",
    "- **Dots** represent outliers.\n",
    "\n",
    "Interpretation:\n",
    "- If the sales boxplot is more spread out than units, it means sales values vary more widely.\n",
    "- Outliers indicate unusually high or low transactions, which may require deeper investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89f345b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution plots saved!\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# DISTRIBUTION PLOTS\n",
    "# ----------------------------------------------\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(df[SALES_COL], kde=True)\n",
    "plt.title(\"Sales Distribution Histogram\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR/\"sales_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(df[UNIT_COL], kde=True)\n",
    "plt.title(\"Units Sold Distribution Histogram\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR/\"units_distribution.png\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Distribution plots saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e0fd52",
   "metadata": {},
   "source": [
    "## Distribution Analysis\n",
    "\n",
    "Histogram + KDE plots were used to understand the distribution of Sales and Units.\n",
    "\n",
    "### Observations:\n",
    "- Skewness reveals whether sales have many small transactions vs large spikes.\n",
    "- Helps identify outliers and typical purchase volumes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b32b20c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Markdown Report Generated Successfully!\n",
      "\n",
      "All outputs saved to: sales_analysis_output\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------\n",
    "# REPORT GENERATION (MARKDOWN)\n",
    "# ----------------------------------------------\n",
    "with open(OUTPUT_DIR/\"summary_report.md\",\"w\") as f:\n",
    "    f.write(\"# Sales Analysis Report\\n\")\n",
    "    f.write(\"Dataset: AusApparalSales4thQrt2020.csv\\n\\n\")\n",
    "    f.write(\"## Detected Columns\\n\")\n",
    "    f.write(f\"- Date Column: {DATE_COL}\\n\")\n",
    "    f.write(f\"- Sales Column: {SALES_COL}\\n\")\n",
    "    f.write(f\"- Units Column: {UNIT_COL}\\n\")\n",
    "    f.write(f\"- State Column: {STATE_COL}\\n\")\n",
    "    f.write(f\"- Group Column: {GROUP_COL}\\n\\n\")\n",
    "\n",
    "    f.write(\"## Top States by Sales\\n\")\n",
    "    f.write(state_sales.head().to_markdown() + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"## Top Groups by Sales\\n\")\n",
    "    f.write(group_sales.head().to_markdown() + \"\\n\\n\")\n",
    "\n",
    "print(\"\\nMarkdown Report Generated Successfully!\")\n",
    "print(\"\\nAll outputs saved to:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdac1ece",
   "metadata": {},
   "source": [
    "## Why Seaborn Was Used\n",
    "\n",
    "Seaborn provides a high-level interface for attractive and informative statistical graphics.\n",
    "\n",
    "Advantages:\n",
    "- Built-in themes for cleaner visual appearance\n",
    "- Simple API for boxplots, histograms, heatmaps\n",
    "- Integrates tightly with pandas DataFrames\n",
    "- Reduces code complexity compared to Matplotlib\n",
    "\n",
    "Because the project involves statistical analysis, Seaborn is the most appropriate tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05ab31",
   "metadata": {},
   "source": [
    "## Recommendation: Use of GroupBy\n",
    "\n",
    "The `groupby()` method is highly recommended because:\n",
    "\n",
    "- It is optimized for large datasets.\n",
    "- Produces meaningful summaries (sum, mean, count) with one line of code.\n",
    "- Reduces risk of manual errors.\n",
    "- Enables efficient state-wise and group-wise sales reporting.\n",
    "\n",
    "Therefore, groupby is the preferred approach for aggregated reporting in this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6b3116",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "### Key Findings\n",
    "- The highest sales come from specific states and customer groups identified in the analysis.\n",
    "- Daily, weekly, and monthly trends show consistent patterns that can be used for forecasting.\n",
    "- Peak-hour analysis highlights when customers are most likely to shop.\n",
    "\n",
    "### Business Recommendations\n",
    "1. **Increase marketing during peak hours** to capitalize on high engagement.\n",
    "2. **Target top-performing states and customer groups** with loyalty programs.\n",
    "3. **Use low-sales hours** to run flash discounts or restock operations.\n",
    "4. **Track monthly and quarterly trends** for inventory forecasting.\n",
    "5. **Investigate outliers** to identify bulk orders or anomalies.\n",
    "\n",
    "### Conclusion\n",
    "The dataset provides strong insights into customer behavior and time-based sales patterns.\n",
    "These insights can guide operational decisions, marketing strategy, and resource allocation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
